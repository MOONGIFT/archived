---
layout: post
title: "マナーを守るプログラムに。robots.txtを解釈する.NETライブラリ「RobotsTxt」"
date: 2011-09-19T06:00:00+09:00
categories:
- ソフトウェア
- Windows
- Web
- オープンソース
tags: 
- ライセンス - MIT License
- ホスティング - Google Code
- 開発 - ライブラリ
- 開発
- 技術 - ネットワーク
permalink: /2011/09/20110919/
catch: https://images.moongift.jp/2011/09/3a718a46c68fe16b53f9fa82ae6c034b.png
id: 29439
---
RobotsTxtはrobots.txtを解釈する.NET向けライブラリ。

  

RobotsTxtはWindows用のオープンソース・ソフトウェア。インターネットに関係するソフトウェアを開発しているとクローラーが必要になることがある。そういう時、目的にぴったりマッチしたものは見つからず自作することが多い。

  

[![](https://images.moongift.jp/2011/09/3a718a46c68fe16b53f9fa82ae6c034b.png)](https://images.moongift.jp/2011/09/90c0e89a4c764d71b0edf1936af6f864.png)  
**使い方の例**

  

そんな時、必要になるのがrobots.txtを解釈するプログラムだ。もちろん行儀の悪いクローラーも多いが、適切なサービス/ソフトウェアを作るならばrobots.txtを判断すべきだろう。そんな時、WindowsアプリケーションであればRobotsTxtが利用できそうだ。

  
<!--more-->  

RobotsTxtはDLL形式で提供されるライブラリで、robots.txtのパーサーだ。まず最初にコンテンツ（テキスト）を読み込ませる。その上でユーザエージェントとアクセスしたいパスを与えるとアクセス可否が返ってくる仕組みだ。シンプルで分かりやすい。

  

さらにクローラーの頻度を設定する仕組みの判断にも対応している。ミリ秒単位で返ってくるので、設定されていればその間が処理を停止してアクセス過多にならないようにすべきだろう。その他Sitemapやワイルドカード指定にも対応している。コンテンツを取得するようなソフトウェアを開発する際には使ってみよう。

  
  
  

**MOONGIFTはこう見る**

  

robots.txtを設置する側にも一定の知識が必要ではあるが、やはりクローリングする側が適切にコンテンツを集積できるように考えなければならない。過剰な負荷は以前あった図書館サイトにアクセスした結果、警察沙汰になったというような結果になりかねない。

  

MOONGIFTにも時折異常な頻度でアクセスしてくるプログラムなどがあるが、robots.txtに書いた所でまず意味はない。大抵、.htaccessやプログラム側で弾かざるを得ない。そうした労力を相手に押し付けるようなソフトウェアはプログラマーとしては避けるべきだろう。マナーを知る上でもチェックしたいソフトウェアだ。

  

[robotstxt - A robots.txt parser in C# - Google Project Hosting](http://code.google.com/p/robotstxt/)

